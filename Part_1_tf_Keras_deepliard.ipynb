{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhmDzfIRaHOJEMRWiQBj98",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robsub/Keras/blob/main/Part_1_tf_Keras_deepliard.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get (in this instance create) and pre-process the data"
      ],
      "metadata": {
        "id": "5w4wdU-gSDGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from random import randint\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "metadata": {
        "id": "1N1YY0YrFchX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HXWcekcnEfC9"
      },
      "outputs": [],
      "source": [
        "# create lists to hold samples and labels\n",
        "\n",
        "train_labels = []\n",
        "train_samples = []"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example data:\n",
        "\n",
        "- An experimental drug was tested on infivifuals from ages 13 to 100 in a clinical trial. \n",
        "\n",
        "- The trial had 2100 participants. Half were under 65 years old, half were under 65 years or older. \n",
        "\n",
        "- Around 95% of patients 65 or older experienced side effects. \n",
        "\n",
        "- Around 95% of patients under 65% experienced no side effects. "
      ],
      "metadata": {
        "id": "WHpqlDb_GzLX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "fit(\n",
        "    x=None,\n",
        "    y=None,\n",
        "    batch_size=None,\n",
        "    epochs=1,\n",
        "    verbose='auto',\n",
        "    callbacks=None,\n",
        "    validation_split=0.0,\n",
        "    validation_data=None,\n",
        "    shuffle=True,\n",
        "    class_weight=None,\n",
        "    sample_weight=None,\n",
        "    initial_epoch=0,\n",
        "    steps_per_epoch=None,\n",
        "    validation_steps=None,\n",
        "    validation_batch_size=None,\n",
        "    validation_freq=1,\n",
        "    max_queue_size=10,\n",
        "    workers=1,\n",
        "    use_multiprocessing=False\n",
        ")\n",
        "\n",
        "Trains the model for a fixed number of epochs (iterations on a dataset)."
      ],
      "metadata": {
        "id": "TBks0D5Vl6w_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.tensorflow.org/api_docs/python/tf/keras/Sequential X and Y lets use a numpy array (y must match x format)\n",
        "\n",
        "\n",
        "for i in range(50):\n",
        "# 5% younger individuals who did exp side effects\n",
        "  random_younger = randint(13,64)\n",
        "  train_samples.append(random_younger)\n",
        "  train_labels.append(1) # 1 = side effects\n",
        "\n",
        "# 5% older individuals who did not exp side effects\n",
        "  random_older = randint(65,100)\n",
        "  train_samples.append(random_older)\n",
        "  train_labels.append(0) # 0 = no side effects\n",
        "\n",
        "for i in range(1000):\n",
        "# 95% younger individuals who did not exp side effects\n",
        "  random_younger = randint(13,64)\n",
        "  train_samples.append(random_younger)\n",
        "  train_labels.append(0)\n",
        "\n",
        "# 95% older individuals who did exp side effects\n",
        "  random_older = randint(65,100)\n",
        "  train_samples.append(random_older)\n",
        "  train_labels.append(1)\n",
        "\n"
      ],
      "metadata": {
        "id": "e-BRT8yJSDla"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in train_samples[:5]: # print first 5 only\n",
        "    print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aPFDGIcp-Fk",
        "outputId": "2fe6feb6-308c-4868-ed5b-cecf80f30b44"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "39\n",
            "64\n",
            "23\n",
            "58\n",
            "71\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in train_labels[:5]:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8F96u8wPGxUA",
        "outputId": "72e5f6c3-c23b-40e1-c276-09cae3c77e63"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LEts process the data to be piped into something the fit command expects (lets go with a numpy array)\n",
        "\n",
        "train_labels = np.array(train_labels) # This line is converting the train_labels list to a numpy array. Numpy is a numerical computing library for Python and numpy.array() is a function that creates a new numpy array from a given list.\n",
        "train_samples = np.array(train_samples) # This line is converting the train_samples list to a numpy array.\n",
        "train_labels,train_samples = shuffle(train_labels, train_samples) # This line is shuffling the two numpy arrays train_labels and train_samples together using the shuffle() function from the sklearn.utils module. This is a common technique used in machine learning to randomize the order of the training data so that the model does not learn any order-related biases. The function returns two shuffled arrays, which are then unpacked and assigned to the original variable names train_labels and train_samples.\n",
        "\n",
        "# Check the type of the variables after conversion\n",
        "print(type(train_labels))   # Output: <class 'numpy.ndarray'>\n",
        "print(type(train_samples))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1UNp78Fp3-s",
        "outputId": "6ed57c10-afaf-43fa-92d3-101a5ec445e5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This code is scaling the train_samples data to normalize and standardize it, which is a common technique used in machine learning to improve model performance.\n",
        "\n",
        "# The MinMaxScaler is a scaler object from the sklearn.preprocessing module that scales data to a specified range, which is [0,1] in this case. The feature_range parameter specifies the desired range.\n",
        "\n",
        "# train_samples is reshaped to a 2D numpy array using the reshape() function with -1 as the first parameter and 1 as the second parameter. This is necessary because the fit_transform() method of MinMaxScaler expects a 2D array as input.\n",
        "\n",
        "# The fit_transform() method of MinMaxScaler is then called on the reshaped train_samples data to fit the scaler to the data and transform it to the specified range. The resulting scaled data is assigned to the scaled_train_samples variable.\n",
        "\n",
        "# AKA minmax scaler object used to create feature_range, feature range is then used on next line to rescale data from current range which is 13 to 100 down to a scale of 0 to 1. and the last bit ((train_samples.reshape(-1,1))) is changing data from 1 dimensional to 2 dimensional which is what is expected by the fit function. \n",
        "scaler = MinMaxScaler(feature_range=(0,1))\n",
        "scaled_train_samples = scaler.fit_transform(train_samples.reshape(-1,1))"
      ],
      "metadata": {
        "id": "KEDPpaK7szXw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in scaled_train_samples[:5]:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySsJ5shCvrMy",
        "outputId": "10cf554a-f844-4b9d-d929-544dc75fc43a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.29885057]\n",
            "[0.5862069]\n",
            "[0.11494253]\n",
            "[0.51724138]\n",
            "[0.66666667]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple tf.keras Sequential Model"
      ],
      "metadata": {
        "id": "k-l_8CJ1yl84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf # to BUILD our first model\n",
        "from tensorflow import keras # to BUILD our first model\n",
        "from tensorflow.keras.models import Sequential # to BUILD our first model\n",
        "from tensorflow.keras.layers import Activation, Dense # to BUILD our first model\n",
        "from tensorflow.keras.optimizers import Adam # to TRAIN our first model\n",
        "from tensorflow.keras.metrics import categorical_crossentropy # to TRAIN our first model\n",
        "\n",
        "# tensorflow is an open-source software library for dataflow and differentiable programming across a range of tasks. It is commonly used for building deep learning models.\n",
        "# keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It is designed to enable fast experimentation with deep neural networks, and thus it is commonly used for building and training neural networks.\n",
        "# Sequential is a Keras class for creating a sequential neural network model.\n",
        "# Activation is a Keras class that specifies the activation function to be used in a neural network layer.\n",
        "# Dense is a Keras class that specifies a fully connected neural network layer.\n",
        "# Adam is a Keras class that specifies the optimizer used for training a neural network model.\n",
        "# categorical_crossentropy is a Keras metric used to calculate the loss of a classification model during training.\n",
        "# You will use these libraries and classes to define, compile, and train your neural network model for your specific machine learning task.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Je6eSvCryq84"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the sequential model (sequential is the simplest type of model using keras or tensorflow)\n",
        "# The sequential model architecture is a type of feedforward neural network, which means the information flows only in one direction, from input to output.\n",
        "# Yes, that is correct. Even though backpropagation propagates the error from right to left, a neural network model that consists of only forward-pass layers, like a Sequential model in Keras, is still considered a linear model because the output of each layer is fed directly into the next layer, without any recurrent connections that would allow feedback from later layers to earlier layers. Therefore, the model can be thought of as processing its input in a linear, sequential manner, one layer at a time, from left to right.\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(units=16, input_shape=(1,), activation='relu'),\n",
        "    Dense(units=32, activation='relu'),\n",
        "    Dense(units=2, activation='softmax'),\n",
        "])\n",
        "\n",
        "# This code is defining a sequential model with three dense layers:\n",
        "\n",
        "# The first dense layer has 16 units/neurons, and takes an input of shape (1,) (a single input value) and applies the rectified linear unit (ReLU) activation function.\n",
        "# The second dense layer has 32 units/neurons and applies the ReLU activation function.\n",
        "# The third dense layer has 2 units/neurons and applies the softmax activation function.\n",
        "# The first dense layer is defined to have an input shape of (1,) because the input to the model will be a single value, and the output will be one of two classes. The final dense layer has 2 units/neurons because there are two classes to predict.\n",
        "\n",
        "# So overall, the model takes a single input value and applies two hidden layers with ReLU activation, followed by a final output layer with a softmax activation to predict one of two classes.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Rob Field\n",
        "# What is a dense layer? What other types of layers exist and could be used in sequential NN?\n",
        "\n",
        "# A dense layer is a type of layer in a neural network where each neuron is fully connected to all neurons in the previous layer. In a dense layer, each neuron computes a weighted sum of its inputs, adds a bias term, and applies an activation function to the result.\n",
        "\n",
        "# Other types of layers that can be used in a sequential neural network include:\n",
        "\n",
        "# Convolutional layers: these layers are used for image processing and feature extraction. They work by applying a convolution operation on the input, which is a sliding window that extracts features from the input.\n",
        "\n",
        "# Pooling layers: these layers are used in conjunction with convolutional layers to reduce the dimensionality of the feature maps and provide translation invariance.\n",
        "\n",
        "# Recurrent layers: these layers are used for processing sequential data, such as text or time series. They work by maintaining an internal state that is updated at each time step and can retain information from previous time steps.\n",
        "\n",
        "# Dropout layers: these layers are used to prevent overfitting by randomly dropping out a fraction of the neurons during training.\n",
        "\n",
        "# Batch normalization layers: these layers are used to normalize the inputs to each layer and speed up training by reducing the internal covariate shift.\n",
        "\n",
        "# Embedding layers: these layers are used for natural language processing tasks, such as text classification or sentiment analysis. They work by mapping each word in the input text to a dense vector representation.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# An activation function is a mathematical function applied to the output of a neural network layer. It introduces non-linearity into the model and allows it to learn more complex patterns.\n",
        "\n",
        "# ReLU (Rectified Linear Unit) is a commonly used activation function in deep learning that returns 0 for negative inputs and the input value for non-negative inputs.\n",
        "\n",
        "# Softmax is an activation function used in the final layer of a neural network to normalize the output into a probability distribution over multiple classes. It forces the sum of the output values to be equal to 1 and makes it easier to interpret the output as probabilities.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Rob Field\n",
        "# When you say equal to 1, do you mean between 0 and 1?\n",
        "# Yes, that's correct. The softmax function is used to convert the outputs of the neural network into a probability distribution, where each output is a number between 0 and 1, and the sum of all outputs is equal to 1.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Rob Field\n",
        "# sum of all outputs is equal to 1. - can you explain?\n",
        "# The softmax activation function transforms the output of a layer into a probability distribution over the predicted classes. The function takes in a vector of arbitrary real-valued scores and squashes them down to a range between 0 and 1, so that they can be interpreted as probabilities.\n",
        "\n",
        "# The output of a softmax function is a vector of the same length as the input, where each element represents the probability of the input belonging to a particular class. The sum of all the elements in the output vector is always equal to 1, ensuring that the outputs can be interpreted as probabilities that an input belongs to each class.\n"
      ],
      "metadata": {
        "id": "2UoVdo844Ios"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}