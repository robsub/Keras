{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMcllA3JtSyw3iIW3nhzKol",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robsub/Keras/blob/main/Part_1_tf_Keras_deepliard.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get (in this instance create) and pre-process the data"
      ],
      "metadata": {
        "id": "5w4wdU-gSDGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from random import randint\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "metadata": {
        "id": "1N1YY0YrFchX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HXWcekcnEfC9"
      },
      "outputs": [],
      "source": [
        "# create lists to hold samples and labels\n",
        "\n",
        "train_labels = []\n",
        "train_samples = []"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example data:\n",
        "\n",
        "- An experimental drug was tested on infivifuals from ages 13 to 100 in a clinical trial. \n",
        "\n",
        "- The trial had 2100 participants. Half were under 65 years old, half were under 65 years or older. \n",
        "\n",
        "- Around 95% of patients 65 or older experienced side effects. \n",
        "\n",
        "- Around 95% of patients under 65% experienced no side effects. "
      ],
      "metadata": {
        "id": "WHpqlDb_GzLX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "fit(\n",
        "    x=None,\n",
        "    y=None,\n",
        "    batch_size=None,\n",
        "    epochs=1,\n",
        "    verbose='auto',\n",
        "    callbacks=None,\n",
        "    validation_split=0.0,\n",
        "    validation_data=None,\n",
        "    shuffle=True,\n",
        "    class_weight=None,\n",
        "    sample_weight=None,\n",
        "    initial_epoch=0,\n",
        "    steps_per_epoch=None,\n",
        "    validation_steps=None,\n",
        "    validation_batch_size=None,\n",
        "    validation_freq=1,\n",
        "    max_queue_size=10,\n",
        "    workers=1,\n",
        "    use_multiprocessing=False\n",
        ")\n",
        "\n",
        "Trains the model for a fixed number of epochs (iterations on a dataset)."
      ],
      "metadata": {
        "id": "TBks0D5Vl6w_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.tensorflow.org/api_docs/python/tf/keras/Sequential X and Y lets use a numpy array (y must match x format)\n",
        "\n",
        "\n",
        "for i in range(50):\n",
        "# 5% younger individuals who did exp side effects\n",
        "  random_younger = randint(13,64)\n",
        "  train_samples.append(random_younger)\n",
        "  train_labels.append(1) # 1 = side effects\n",
        "\n",
        "# 5% older individuals who did not exp side effects\n",
        "  random_older = randint(65,100)\n",
        "  train_samples.append(random_older)\n",
        "  train_labels.append(0) # 0 = no side effects\n",
        "\n",
        "for i in range(1000):\n",
        "# 95% younger individuals who did not exp side effects\n",
        "  random_younger = randint(13,64)\n",
        "  train_samples.append(random_younger)\n",
        "  train_labels.append(0)\n",
        "\n",
        "# 95% older individuals who did exp side effects\n",
        "  random_older = randint(65,100)\n",
        "  train_samples.append(random_older)\n",
        "  train_labels.append(1)\n",
        "\n"
      ],
      "metadata": {
        "id": "e-BRT8yJSDla"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in train_samples[:5]: # print first 5 only\n",
        "    print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aPFDGIcp-Fk",
        "outputId": "2fe6feb6-308c-4868-ed5b-cecf80f30b44"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "39\n",
            "64\n",
            "23\n",
            "58\n",
            "71\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in train_labels[:5]:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8F96u8wPGxUA",
        "outputId": "72e5f6c3-c23b-40e1-c276-09cae3c77e63"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LEts process the data to be piped into something the fit command expects (lets go with a numpy array)\n",
        "\n",
        "train_labels = np.array(train_labels) # This line is converting the train_labels list to a numpy array. Numpy is a numerical computing library for Python and numpy.array() is a function that creates a new numpy array from a given list.\n",
        "train_samples = np.array(train_samples) # This line is converting the train_samples list to a numpy array.\n",
        "train_labels,train_samples = shuffle(train_labels, train_samples) # This line is shuffling the two numpy arrays train_labels and train_samples together using the shuffle() function from the sklearn.utils module. This is a common technique used in machine learning to randomize the order of the training data so that the model does not learn any order-related biases. The function returns two shuffled arrays, which are then unpacked and assigned to the original variable names train_labels and train_samples.\n",
        "\n",
        "# Check the type of the variables after conversion\n",
        "print(type(train_labels))   # Output: <class 'numpy.ndarray'>\n",
        "print(type(train_samples))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1UNp78Fp3-s",
        "outputId": "6ed57c10-afaf-43fa-92d3-101a5ec445e5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This code is scaling the train_samples data to normalize and standardize it, which is a common technique used in machine learning to improve model performance.\n",
        "\n",
        "# The MinMaxScaler is a scaler object from the sklearn.preprocessing module that scales data to a specified range, which is [0,1] in this case. The feature_range parameter specifies the desired range.\n",
        "\n",
        "# train_samples is reshaped to a 2D numpy array using the reshape() function with -1 as the first parameter and 1 as the second parameter. This is necessary because the fit_transform() method of MinMaxScaler expects a 2D array as input.\n",
        "\n",
        "# The fit_transform() method of MinMaxScaler is then called on the reshaped train_samples data to fit the scaler to the data and transform it to the specified range. The resulting scaled data is assigned to the scaled_train_samples variable.\n",
        "\n",
        "# AKA minmax scaler object used to create feature_range, feature range is then used on next line to rescale data from current range which is 13 to 100 down to a scale of 0 to 1. and the last bit ((train_samples.reshape(-1,1))) is changing data from 1 dimensional to 2 dimensional which is what is expected by the fit function. \n",
        "scaler = MinMaxScaler(feature_range=(0,1))\n",
        "scaled_train_samples = scaler.fit_transform(train_samples.reshape(-1,1))"
      ],
      "metadata": {
        "id": "KEDPpaK7szXw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in scaled_train_samples[:5]:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySsJ5shCvrMy",
        "outputId": "10cf554a-f844-4b9d-d929-544dc75fc43a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.29885057]\n",
            "[0.5862069]\n",
            "[0.11494253]\n",
            "[0.51724138]\n",
            "[0.66666667]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple tf.keras Sequential Model"
      ],
      "metadata": {
        "id": "k-l_8CJ1yl84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf # to BUILD our first model\n",
        "from tensorflow import keras # to BUILD our first model\n",
        "from tensorflow.keras.models import Sequential # to BUILD our first model\n",
        "from tensorflow.keras.layers import Activation, Dense # to BUILD our first model\n",
        "from tensorflow.keras.optimizers import Adam # to TRAIN our first model\n",
        "from tensorflow.keras.metrics import categorical_crossentropy # to TRAIN our first model\n",
        "\n",
        "# tensorflow is an open-source software library for dataflow and differentiable programming across a range of tasks. It is commonly used for building deep learning models.\n",
        "# keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It is designed to enable fast experimentation with deep neural networks, and thus it is commonly used for building and training neural networks.\n",
        "# Sequential is a Keras class for creating a sequential neural network model.\n",
        "# Activation is a Keras class that specifies the activation function to be used in a neural network layer.\n",
        "# Dense is a Keras class that specifies a fully connected neural network layer.\n",
        "# Adam is a Keras class that specifies the optimizer used for training a neural network model.\n",
        "# categorical_crossentropy is a Keras metric used to calculate the loss of a classification model during training.\n",
        "# You will use these libraries and classes to define, compile, and train your neural network model for your specific machine learning task.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Je6eSvCryq84"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the sequential model (sequential is the simplest type of model using keras or tensorflow)\n",
        "# The sequential model architecture is a type of feedforward neural network, which means the information flows only in one direction, from input to output.\n",
        "# Yes, that is correct. Even though backpropagation propagates the error from right to left, a neural network model that consists of only forward-pass layers, like a Sequential model in Keras, is still considered a linear model because the output of each layer is fed directly into the next layer, without any recurrent connections that would allow feedback from later layers to earlier layers. Therefore, the model can be thought of as processing its input in a linear, sequential manner, one layer at a time, from left to right.\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(units=16, input_shape=(1,), activation='relu'), # hidden layer (not input layer)\n",
        "    Dense(units=32, activation='relu'),\n",
        "    Dense(units=2, activation='softmax'), # output layer\n",
        "])\n",
        "\n",
        "# This code is defining a sequential model with three dense layers:\n",
        "\n",
        "# The first dense layer has 16 units/neurons, and takes an input of shape (1,) (a single input value) and applies the rectified linear unit (ReLU) activation function.\n",
        "# The second dense layer has 32 units/neurons and applies the ReLU activation function.\n",
        "# The third dense layer has 2 units/neurons and applies the softmax activation function.\n",
        "# The first dense layer is defined to have an input shape of (1,) because the input to the model will be a single value, and the output will be one of two classes. The final dense layer has 2 units/neurons because there are two classes to predict.\n",
        "\n",
        "# So overall, the model takes a single input value and applies two hidden layers with ReLU activation, followed by a final output layer with a softmax activation to predict one of two classes.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Rob Field\n",
        "# What is a dense layer? What other types of layers exist and could be used in sequential NN?\n",
        "\n",
        "# A dense layer is a type of layer in a neural network where each neuron is fully connected to all neurons in the previous layer. In a dense layer, each neuron computes a weighted sum of its inputs, adds a bias term, and applies an activation function to the result.\n",
        "\n",
        "# Other types of layers that can be used in a sequential neural network include:\n",
        "\n",
        "# Convolutional layers: these layers are used for image processing and feature extraction. They work by applying a convolution operation on the input, which is a sliding window that extracts features from the input.\n",
        "\n",
        "# Pooling layers: these layers are used in conjunction with convolutional layers to reduce the dimensionality of the feature maps and provide translation invariance.\n",
        "\n",
        "# Recurrent layers: these layers are used for processing sequential data, such as text or time series. They work by maintaining an internal state that is updated at each time step and can retain information from previous time steps.\n",
        "\n",
        "# Dropout layers: these layers are used to prevent overfitting by randomly dropping out a fraction of the neurons during training.\n",
        "\n",
        "# Batch normalization layers: these layers are used to normalize the inputs to each layer and speed up training by reducing the internal covariate shift.\n",
        "\n",
        "# Embedding layers: these layers are used for natural language processing tasks, such as text classification or sentiment analysis. They work by mapping each word in the input text to a dense vector representation.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# An activation function is a mathematical function applied to the output of a neural network layer. It introduces non-linearity into the model and allows it to learn more complex patterns.\n",
        "\n",
        "# ReLU (Rectified Linear Unit) is a commonly used activation function in deep learning that returns 0 for negative inputs and the input value for non-negative inputs.\n",
        "\n",
        "# Softmax is an activation function used in the final layer of a neural network to normalize the output into a probability distribution over multiple classes. It forces the sum of the output values to be equal to 1 and makes it easier to interpret the output as probabilities.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Rob Field\n",
        "# When you say equal to 1, do you mean between 0 and 1?\n",
        "# Yes, that's correct. The softmax function is used to convert the outputs of the neural network into a probability distribution, where each output is a number between 0 and 1, and the sum of all outputs is equal to 1.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Rob Field\n",
        "# sum of all outputs is equal to 1. - can you explain?\n",
        "# The softmax activation function transforms the output of a layer into a probability distribution over the predicted classes. The function takes in a vector of arbitrary real-valued scores and squashes them down to a range between 0 and 1, so that they can be interpreted as probabilities.\n",
        "\n",
        "# The output of a softmax function is a vector of the same length as the input, where each element represents the probability of the input belonging to a particular class. The sum of all the elements in the output vector is always equal to 1, ensuring that the outputs can be interpreted as probabilities that an input belongs to each class.\n",
        "\n",
        "\n",
        "\n",
        "# The input_shape parameter in Keras defines the shape of the input data that will be passed to the neural network model. In the example you provided, the input_shape is (1,), which means that the input data will have one dimension, and the size of that dimension is 1.\n",
        "\n",
        "# In other words, the input data in this case will be a one-dimensional array with one element, such as [3] or [-2]. This is a common input shape for models that take in scalar values as inputs, such as regression models.\n",
        "\n",
        "# Other possible input shapes that could be used depending on the data could be (n,) where n is the number of input features. For example, if the input data was a 2D array with 100 rows and 10 columns, the input shape could be (10,) to indicate that there are 10 features or (100, 10) to indicate there are 100 rows and 10 columns. The choice of input shape depends on the nature of the data and the type of problem being solved.\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UoVdo844Ios",
        "outputId": "bc8c3295-19aa-4fd2-8a4d-200d81d95e16"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_3 (Dense)             (None, 16)                32        \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 32)                544       \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 2)                 66        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 642\n",
            "Trainable params: 642\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Looking at a non sequntial model (BEYOND THE SCOPE OF THIS COURSE SO FAR):\n",
        "\n",
        "here's an example code snippet that uses the functional API and connects layers together by passing the output of one layer as the input to the next layer AKA NOT a sequential/linear model\n",
        "\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Define the input layer\n",
        "input_layer = Input(shape=(784,))\n",
        "\n",
        "# Define the first hidden layer and connect it to the input layer\n",
        "hidden_layer_1 = Dense(units=256, activation='relu')(input_layer)\n",
        "\n",
        "# Define the second hidden layer and connect it to the first hidden layer\n",
        "hidden_layer_2 = Dense(units=128, activation='relu')(hidden_layer_1)\n",
        "\n",
        "# Define the output layer and connect it to the second hidden layer\n",
        "output_layer = Dense(units=10, activation='softmax')(hidden_layer_2)\n",
        "\n",
        "# Define the model using the input and output layers\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "This still looks linear to me (from left to right) where are the loops?\n",
        "\n",
        "You're correct, the code I provided still shows a linear flow from right to left. However, with the functional API, you can create more complex models that involve loops or branches.\n",
        "\n",
        "For example, you can create a loop by using a for loop to repeat a layer multiple times, or you can create a branch by connecting the output of one layer to multiple subsequent layers. These more complex models are often used for tasks such as image segmentation or language translation.\n",
        "\n",
        "Here's an example of how you could create a simple loop using the functional API:\n",
        "\n",
        "scss\n",
        "Copy code\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "input_layer = Input(shape=(1,))\n",
        "hidden_layer = Dense(units=16, activation='relu')(input_layer)\n",
        "for i in range(3):\n",
        "    hidden_layer = Dense(units=16, activation='relu')(hidden_layer)\n",
        "output_layer = Dense(units=2, activation='softmax')(hidden_layer)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "In this example, we use a for loop to create three hidden layers with the same architecture. The output of each hidden layer is connected to the next hidden layer in the loop, creating a loop in the model. The final output layer is connected to the last hidden layer in the loop.\n",
        "\n",
        "*italicized text*"
      ],
      "metadata": {
        "id": "W-hGk8gS-fnK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RPc-BYI487fI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the NN "
      ],
      "metadata": {
        "id": "ejlojJNAJnyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Call model.compile function - prepares the model for training\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "xl4RGofMJ3VD"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x=scaled_train_samples, y=train_labels, batch_size=10, epochs=30, shuffle=True, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asyDngs5KUuR",
        "outputId": "465b0d16-0914-4043-c48c-f8e2cdfccc18"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "210/210 - 1s - loss: 0.3666 - accuracy: 0.8929 - 988ms/epoch - 5ms/step\n",
            "Epoch 2/30\n",
            "210/210 - 0s - loss: 0.3496 - accuracy: 0.8971 - 328ms/epoch - 2ms/step\n",
            "Epoch 3/30\n",
            "210/210 - 0s - loss: 0.3351 - accuracy: 0.9005 - 317ms/epoch - 2ms/step\n",
            "Epoch 4/30\n",
            "210/210 - 0s - loss: 0.3233 - accuracy: 0.9105 - 329ms/epoch - 2ms/step\n",
            "Epoch 5/30\n",
            "210/210 - 0s - loss: 0.3132 - accuracy: 0.9076 - 334ms/epoch - 2ms/step\n",
            "Epoch 6/30\n",
            "210/210 - 0s - loss: 0.3049 - accuracy: 0.9152 - 326ms/epoch - 2ms/step\n",
            "Epoch 7/30\n",
            "210/210 - 0s - loss: 0.2980 - accuracy: 0.9148 - 333ms/epoch - 2ms/step\n",
            "Epoch 8/30\n",
            "210/210 - 0s - loss: 0.2921 - accuracy: 0.9252 - 321ms/epoch - 2ms/step\n",
            "Epoch 9/30\n",
            "210/210 - 0s - loss: 0.2874 - accuracy: 0.9214 - 321ms/epoch - 2ms/step\n",
            "Epoch 10/30\n",
            "210/210 - 0s - loss: 0.2831 - accuracy: 0.9267 - 320ms/epoch - 2ms/step\n",
            "Epoch 11/30\n",
            "210/210 - 0s - loss: 0.2796 - accuracy: 0.9290 - 327ms/epoch - 2ms/step\n",
            "Epoch 12/30\n",
            "210/210 - 0s - loss: 0.2766 - accuracy: 0.9262 - 332ms/epoch - 2ms/step\n",
            "Epoch 13/30\n",
            "210/210 - 0s - loss: 0.2739 - accuracy: 0.9290 - 316ms/epoch - 2ms/step\n",
            "Epoch 14/30\n",
            "210/210 - 0s - loss: 0.2716 - accuracy: 0.9295 - 314ms/epoch - 1ms/step\n",
            "Epoch 15/30\n",
            "210/210 - 0s - loss: 0.2696 - accuracy: 0.9319 - 312ms/epoch - 1ms/step\n",
            "Epoch 16/30\n",
            "210/210 - 0s - loss: 0.2680 - accuracy: 0.9314 - 344ms/epoch - 2ms/step\n",
            "Epoch 17/30\n",
            "210/210 - 0s - loss: 0.2662 - accuracy: 0.9343 - 345ms/epoch - 2ms/step\n",
            "Epoch 18/30\n",
            "210/210 - 0s - loss: 0.2648 - accuracy: 0.9310 - 329ms/epoch - 2ms/step\n",
            "Epoch 19/30\n",
            "210/210 - 0s - loss: 0.2635 - accuracy: 0.9357 - 318ms/epoch - 2ms/step\n",
            "Epoch 20/30\n",
            "210/210 - 0s - loss: 0.2622 - accuracy: 0.9314 - 336ms/epoch - 2ms/step\n",
            "Epoch 21/30\n",
            "210/210 - 0s - loss: 0.2612 - accuracy: 0.9357 - 339ms/epoch - 2ms/step\n",
            "Epoch 22/30\n",
            "210/210 - 0s - loss: 0.2600 - accuracy: 0.9357 - 363ms/epoch - 2ms/step\n",
            "Epoch 23/30\n",
            "210/210 - 1s - loss: 0.2593 - accuracy: 0.9357 - 532ms/epoch - 3ms/step\n",
            "Epoch 24/30\n",
            "210/210 - 0s - loss: 0.2584 - accuracy: 0.9357 - 488ms/epoch - 2ms/step\n",
            "Epoch 25/30\n",
            "210/210 - 0s - loss: 0.2574 - accuracy: 0.9357 - 493ms/epoch - 2ms/step\n",
            "Epoch 26/30\n",
            "210/210 - 0s - loss: 0.2567 - accuracy: 0.9357 - 486ms/epoch - 2ms/step\n",
            "Epoch 27/30\n",
            "210/210 - 0s - loss: 0.2558 - accuracy: 0.9357 - 480ms/epoch - 2ms/step\n",
            "Epoch 28/30\n",
            "210/210 - 0s - loss: 0.2552 - accuracy: 0.9348 - 393ms/epoch - 2ms/step\n",
            "Epoch 29/30\n",
            "210/210 - 0s - loss: 0.2545 - accuracy: 0.9357 - 334ms/epoch - 2ms/step\n",
            "Epoch 30/30\n",
            "210/210 - 0s - loss: 0.2538 - accuracy: 0.9371 - 315ms/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f46341ff7c0>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is being cached to cause the first epoch to take 6ms and the rest 2ms (the first time round - changed since writing this)? Presumably, something is being cached? Do you know what is? AKA What does the epoch need to do only once on the first epoch?\n",
        "\n",
        "The time taken to complete an epoch in a neural network model can depend on several factors such as the size of the dataset, the complexity of the model, and the computing resources available.\n",
        "\n",
        "In this case, it is possible that the first epoch took longer because the data needed to be loaded into the cache or memory. The subsequent epochs may have taken less time because the data was already loaded into the cache or memory and did not need to be reloaded. This can result in faster processing times for the subsequent epochs.\n",
        "\n",
        "Additionally, the Keras fit() function can perform various initialization steps before the first epoch, such as compiling the model and initializing the optimizer, which may also contribute to the longer execution time for the first epoch.\n",
        "\n",
        "It's worth noting that the differences in execution time between epochs in this example are relatively small, and it's unlikely to have a significant impact on the overall training time.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Rob Field\n",
        "I can see that the accuracy improves on each epoch. This makes sense, but does the accuracy always increase per epoch? or is it because this is a simple dataset and model?\n",
        "\n",
        "The accuracy of a model on a given dataset can increase, decrease or remain unchanged from one epoch to the next. The direction and rate of change in accuracy can depend on various factors, such as the complexity of the model, the size and quality of the training dataset, the choice of optimizer and learning rate, and the stopping criterion used to terminate the training process.\n",
        "\n",
        "In general, when a model is underfitting the data, the accuracy is likely to increase with each epoch as the model adjusts its parameters to better fit the training data. However, when the model starts to overfit the data, the accuracy on the training dataset may continue to improve, but the accuracy on the validation dataset may start to decrease, indicating that the model is no longer generalizing well to new data.\n",
        "\n",
        "In the case of the code snippet you provided, the accuracy is increasing on each epoch, which suggests that the model is improving its ability to fit the training data. However, it's also possible that this is due to the simplicity of the dataset and the model. In more complex scenarios, the accuracy may not increase on every epoch or may plateau at some point, indicating that the model has reached its optimal performance."
      ],
      "metadata": {
        "id": "ZJymVEnvL65D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdqgeYdMKvBv",
        "outputId": "2bbbca27-b134-4564-ec80-cfe33c65a142"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_3 (Dense)             (None, 16)                32        \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 32)                544       \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 2)                 66        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 642\n",
            "Trainable params: 642\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build a Validation Set With TensorFlow's Keras API"
      ],
      "metadata": {
        "id": "p6a877zRRqjG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In machine learning, a validation set is a portion of the dataset used to evaluate the performance of a model during training. In the context of sequential neural networks using Keras, the validation set is typically used to evaluate the model's performance on data that it has not seen before.\n",
        "\n",
        "During the training process, the model is typically trained on a portion of the dataset called the training set. The model's performance on the training set is used to adjust its weights and improve its accuracy. However, to ensure that the model is not overfitting (i.e., performing well on the training set but poorly on new, unseen data), it is important to evaluate the model's performance on a separate dataset that it has not been trained on. This is where the validation set comes in.\n",
        "\n",
        "In Keras, you can specify a validation set by passing the validation_data argument to the fit() function. The validation_data argument takes a tuple of inputs and targets (e.g., (x_val, y_val)), which represent the validation data. During training, Keras will evaluate the model's performance on the validation data at the end of each epoch, and this information can be used to monitor the model's progress and adjust its hyperparameters (e.g., learning rate, batch size, etc.) to improve its accuracy."
      ],
      "metadata": {
        "id": "NBceA0qOTOG_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-66GoV_-MQFW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}